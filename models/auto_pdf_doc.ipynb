{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ddfddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      " - MPNet ì„ë² ë”©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - EasyOCR (ì´ë¯¸ì§€)...\n",
      "âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
      "ğŸ–¼ï¸  ì´ë¯¸ì§€ â†’ EasyOCR\n",
      "âœ… EasyOCR: 336ì\n",
      "ğŸ¯ ìµœì¢… íŒì •: ì¼ë°˜ âŒ\n",
      "ğŸ“Š similarity: 0.4827\n",
      "ğŸ“ˆ rule_score: 0.4327\n",
      "ğŸ” í‚¤ì›Œë“œ: 0\n",
      "ğŸ“„ í…ìŠ¤íŠ¸: ë¬¸ì„œí™•ì¸ë²ˆí˜¸ : 1761-2836-1342-6760 ì§„ ë³¸ 2025/10/24 14,26.55 ì œ 2025- 00205325í˜¸ ì–´í•™ì„±ì  ì‚¬ì „ë“±ë¡ í™•ì¸ì„œ ì„± ëª…: ë°• ì—¬ ê²½ ìƒ ë…„ ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import easyocr\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "        \n",
    "class ChildcareDocumentClassifier:\n",
    "    def __init__(self, similarity_threshold: float = 0.70):\n",
    "        print(\"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        print(\" - MPNet ì„ë² ë”©...\")\n",
    "        self.mpnet_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        self.mpnet_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        self.mpnet_model.eval()\n",
    "        \n",
    "        print(\" - EasyOCR (ì´ë¯¸ì§€)...\")\n",
    "        self.easyocr_reader = easyocr.Reader([\"ko\", \"en\"], gpu=False)\n",
    "\n",
    "        # âœ… ê¸°ì¡´ ë³€ìˆ˜ë“¤ ê·¸ëŒ€ë¡œ ìœ ì§€!\n",
    "        self.KEYWORDS = [\"ë³´ìœ¡êµì‚¬\", \"ë³´ìœ¡\", \"ì˜ìœ ì•„ë³´ìœ¡ë²•\", \"ë³´ìœ¡êµì‚¬ìê²©ì¦\", \"ìœ ì•„êµìœ¡ë²•\"]\n",
    "        self.REFERENCES = [\n",
    "            \"ë³´ìœ¡êµì‚¬\", \"ì˜ìœ ì•„ë³´ìœ¡ë²•ì— ë”°ë¼ ë³´ìœ¡êµì‚¬ì˜\", \"ë³´ìœ¡êµì‚¬ 1ê¸‰\", \"ë³´ìœ¡êµì‚¬ 2ê¸‰\", \"ë³´ìœ¡êµì‚¬ 3ê¸‰\", \"ì˜ìœ ì•„ë³´ìœ¡ë²•\", \"ìœ ì•„êµìœ¡ë²•\"\n",
    "        ]\n",
    "        \n",
    "        self.threshold = similarity_threshold\n",
    "        print(\"âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "\n",
    "    def extract_text_mpnet_chandra(self, file_path: str) -> str:\n",
    "        \"\"\"PyMuPDF + EasyOCR\"\"\"\n",
    "        try:\n",
    "            ext = os.path.splitext(file_path)[1].lower()\n",
    "            \n",
    "            # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° â†’ EasyOCR\n",
    "            if ext in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.webp']:\n",
    "                print(\"ğŸ–¼ï¸  ì´ë¯¸ì§€ â†’ EasyOCR\")\n",
    "                image = Image.open(file_path).convert(\"RGB\")\n",
    "                result = self.easyocr_reader.readtext(np.array(image))\n",
    "                text = \" \".join([txt for _, txt, conf in result if conf > 0.3])\n",
    "                text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "                print(f\"âœ… EasyOCR: {len(text)}ì\")\n",
    "                return text\n",
    "            \n",
    "            # PDF íŒŒì¼ì¸ ê²½ìš° â†’ PyMuPDF í…ìŠ¤íŠ¸ ìš°ì„ \n",
    "            elif ext == '.pdf':\n",
    "                print(\"ğŸ“„ PDF â†’ PyMuPDF ì²˜ë¦¬...\")\n",
    "                doc = fitz.open(file_path)\n",
    "                \n",
    "                # 1. í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì¶”ì¶œ (ì²« 3í˜ì´ì§€)\n",
    "                text_chunks = []\n",
    "                for page_num in range(min(3, len(doc))):\n",
    "                    page = doc[page_num]\n",
    "                    text = page.get_text()\n",
    "                    if text.strip():\n",
    "                        text_chunks.append(text)\n",
    "                \n",
    "                full_text = re.sub(r\"\\s+\", \" \", \" \".join(text_chunks)).strip()\n",
    "                print(f\"âœ… PyMuPDF í…ìŠ¤íŠ¸: {len(full_text)}ì\")\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ì¶©ë¶„í•˜ë©´ ë°”ë¡œ ë°˜í™˜\n",
    "                if len(full_text) > 30:\n",
    "                    doc.close()\n",
    "                    return full_text\n",
    "                \n",
    "                # 2. ìŠ¤ìº” PDFì¼ ê²½ìš° â†’ PyMuPDFë¡œ ì´ë¯¸ì§€ ë Œë”ë§ + EasyOCR\n",
    "                print(\"ğŸ“„ ìŠ¤ìº” PDF ê°ì§€ â†’ PyMuPDF ë Œë”ë§ + EasyOCR...\")\n",
    "                ocr_chunks = []\n",
    "                for page_num in range(min(3, len(doc))):  # ì²« 3í˜ì´ì§€ë§Œ\n",
    "                    page = doc[page_num]\n",
    "                    pix = page.get_pixmap(dpi=150)\n",
    "                    img_data = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                    \n",
    "                    # EasyOCR ì ìš©\n",
    "                    ocr_result = self.easyocr_reader.readtext(np.array(img_data))\n",
    "                    page_ocr = \" \".join([txt for _, txt, conf in ocr_result if conf > 0.3])\n",
    "                    if page_ocr.strip():\n",
    "                        ocr_chunks.append(page_ocr)\n",
    "                \n",
    "                ocr_text = re.sub(r\"\\s+\", \" \", \" \".join(ocr_chunks)).strip()\n",
    "                doc.close()\n",
    "                print(f\"âœ… PyMuPDF+EasyOCR: {len(ocr_text)}ì\")\n",
    "                return ocr_text if ocr_text else os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            else:\n",
    "                print(f\"âŒ ë¯¸ì§€ì› í˜•ì‹: {ext}\")\n",
    "                return os.path.splitext(os.path.basename(file_path))[0]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            return os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "\n",
    "    def mpnet_encode(self, sentences: list) -> torch.Tensor:\n",
    "        if len(sentences) == 0:\n",
    "            return torch.empty((0, 768))\n",
    "        encoded_input = self.mpnet_tokenizer(sentences, padding=True, truncation=True, max_length=384, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            model_output = self.mpnet_model(**encoded_input)\n",
    "            embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    def preprocess_sentences(self, text: str) -> list:\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£.!?0-9]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        text = re.sub(r\"(ìŠµë‹ˆë‹¤\\.|ì…ë‹ˆë‹¤\\.|ë‹¤\\.|ìš”\\.)\", r\"\\1<SPLIT>\", text)\n",
    "        text = re.sub(r\"([\\.!?])\", r\"\\1<SPLIT>\", text)\n",
    "        sentences = text.split(\"<SPLIT>\")\n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "\n",
    "    def compute_mpnet_similarity(self, ref_embedding: torch.Tensor, doc_embeddings: torch.Tensor) -> tuple:\n",
    "        if doc_embeddings.numel() == 0:\n",
    "            return 0.0, 0\n",
    "        similarities = torch.nn.functional.cosine_similarity(ref_embedding.unsqueeze(0), doc_embeddings, dim=1)\n",
    "        return float(similarities.max()), int(similarities.argmax())\n",
    "\n",
    "    def classify_childcare_document(self, file_path: str, reference_sentences: list) -> dict:\n",
    "        result = {\n",
    "            \"verdict\": False, \"max_confidence\": 0.0, \"matched_sentence\": \"\",\n",
    "            \"total_sentences\": 0, \"raw_text\": \"\", \"reason\": \"\"\n",
    "        }\n",
    "        \n",
    "        raw_text = self.extract_text_mpnet_chandra(file_path)\n",
    "        result[\"raw_text\"] = raw_text\n",
    "        \n",
    "        doc_sentences = self.preprocess_sentences(raw_text)\n",
    "        result[\"total_sentences\"] = len(doc_sentences)\n",
    "        \n",
    "        if len(doc_sentences) == 0:\n",
    "            result[\"reason\"] = \"ë¬¸ì¥ ì—†ìŒ\"\n",
    "            return result\n",
    "\n",
    "        all_sentences = reference_sentences + doc_sentences\n",
    "        all_embeddings = self.mpnet_encode(all_sentences)\n",
    "        \n",
    "        ref_embeddings = all_embeddings[:len(reference_sentences)]\n",
    "        doc_embeddings = all_embeddings[len(reference_sentences):]\n",
    "\n",
    "        best_matches = []\n",
    "        for i, ref_emb in enumerate(ref_embeddings):\n",
    "            score, doc_idx = self.compute_mpnet_similarity(ref_emb, doc_embeddings)\n",
    "            best_matches.append({\n",
    "                \"ref_idx\": i, \"score\": score, \"matched_sentence\": doc_sentences[doc_idx]\n",
    "            })\n",
    "\n",
    "        max_match = max(best_matches, key=lambda x: x[\"score\"])\n",
    "        result[\"max_confidence\"] = round(max_match[\"score\"], 4)\n",
    "        result[\"matched_sentence\"] = max_match[\"matched_sentence\"]\n",
    "        result[\"verdict\"] = max_match[\"score\"] >= self.threshold\n",
    "        \n",
    "        return result\n",
    "    def classify_childcare_with_rules(self, file_path: str, base_threshold=0.70):\n",
    "        \"\"\"í´ë˜ìŠ¤ ë©”ì„œë“œë¡œ ë³€ê²½ - selfê°€ ì²« ë²ˆì§¸ ì¸ì\"\"\"\n",
    "        result = self.classify_childcare_document(file_path, self.REFERENCES)\n",
    "        score = result.get(\"max_confidence\", 0.0)\n",
    "        raw_text = result.get(\"raw_text\", \"\")\n",
    "        \n",
    "        # KEYWORDSë¥¼ self.KEYWORDSë¡œ ì°¸ì¡°\n",
    "        hit_count = sum(raw_text.count(kw) for kw in self.KEYWORDS)\n",
    "        score += 0.02 * hit_count if hit_count > 0 else -0.05\n",
    "        \n",
    "        result[\"rule_score\"] = round(score, 4)\n",
    "        result[\"verdict\"] = score >= base_threshold\n",
    "        result[\"keyword_hit_count\"] = hit_count\n",
    "        return result\n",
    "    \n",
    "    def save_model(self, save_path: str = \"childcare_model.pkl\"):\n",
    "        \"\"\"ğŸ”¥ ëª¨ë¸ ì „ì²´ ì €ì¥ (pickle)\"\"\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "        file_size = os.path.getsize(save_path) / 1024 / 1024\n",
    "        print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path} ({file_size:.1f}MB)\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, load_path: str):\n",
    "        \"\"\"ğŸ“¥ ëª¨ë¸ ì „ì²´ ë¡œë“œ (pickle)\"\"\"\n",
    "        print(f\"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {load_path}\")\n",
    "        \n",
    "        with open(load_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return classifier\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    classifier = ChildcareDocumentClassifier(0.70)\n",
    "    \n",
    "    # âœ… ë‹¨ì¼ pathë¡œ ì´ë¯¸ì§€/PDF ëª¨ë‘ ì²˜ë¦¬!\n",
    "    test_path = r\"C:\\Users\\ehjun\\Documents\\ìº¡ìŠ¤í†¤ë””ìì¸\\ìë™ë¬¸ì„œì¸ì‹ëª¨ë¸\\data\\ë³´ìœ¡ìê²©ì¦ ì•„ë‹Œê²ƒ\\í† ìµì„±ì .png\"  # ì´ë¯¸ì§€\n",
    "    # test_path = r\"C:\\Users\\ehjun\\Documents\\...(í”„ë¡œì íŠ¸ì‹ ì²­ì„œ.pdf\"  # PDF\n",
    "    \n",
    "    result = classifier.classify_childcare_with_rules(test_path)\n",
    "    \n",
    "    print(\"ğŸ¯ ìµœì¢… íŒì •:\", \"ë³´ìœ¡ âœ…\" if result[\"verdict\"] else \"ì¼ë°˜ âŒ\")\n",
    "    print(f\"ğŸ“Š similarity: {result['max_confidence']}\")\n",
    "    print(f\"ğŸ“ˆ rule_score: {result['rule_score']}\")\n",
    "    print(f\"ğŸ” í‚¤ì›Œë“œ: {result['keyword_hit_count']}\")\n",
    "    print(f\"ğŸ“„ í…ìŠ¤íŠ¸: {result['raw_text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eaf7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸  ì´ë¯¸ì§€ â†’ EasyOCR\n",
      "âœ… EasyOCR: 336ì\n",
      "ğŸ¯ ìµœì¢… íŒì •: ì¼ë°˜ âŒ\n",
      "ğŸ“Š similarity: 0.4827\n",
      "ğŸ“ˆ rule_score: 0.4327\n",
      "ğŸ” í‚¤ì›Œë“œ: 0\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 2: í…ŒìŠ¤íŠ¸ =====\n",
    "test_path = r\"C:\\Users\\ehjun\\Documents\\ìº¡ìŠ¤í†¤ë””ìì¸\\ìë™ë¬¸ì„œì¸ì‹ëª¨ë¸\\data\\ë³´ìœ¡ìê²©ì¦ ì•„ë‹Œê²ƒ\\í† ìµì„±ì .png\"\n",
    "\n",
    "result = classifier.classify_childcare_with_rules(test_path)\n",
    "\n",
    "print(\"ğŸ¯ ìµœì¢… íŒì •:\", \"ë³´ìœ¡ âœ…\" if result[\"verdict\"] else \"ì¼ë°˜ âŒ\")\n",
    "print(f\"ğŸ“Š similarity: {result['max_confidence']}\")\n",
    "print(f\"ğŸ“ˆ rule_score: {result['rule_score']}\")\n",
    "print(f\"ğŸ” í‚¤ì›Œë“œ: {result['keyword_hit_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9f754b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: childcare_model.pkl (505.5MB)\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 3: ëª¨ë¸ ì €ì¥ =====\n",
    "classifier.save_model(\"childcare_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ëª¨ë¸ ë¡œë”©: childcare_model.pkl\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "âœ… ë¡œë“œ ì™„ë£Œ! ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 4: ëª¨ë¸ ë¡œë“œ=====\n",
    "loaded_classifier = ChildcareDocumentClassifier.load_model(\"childcare_model.pkl\")\n",
    "print(\"âœ… ë¡œë“œ ì™„ë£Œ! ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "358afdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ PDF â†’ PyMuPDF ì²˜ë¦¬...\n",
      "âœ… PyMuPDF í…ìŠ¤íŠ¸: 0ì\n",
      "ğŸ“„ ìŠ¤ìº” PDF ê°ì§€ â†’ PyMuPDF ë Œë”ë§ + EasyOCR...\n",
      "âœ… PyMuPDF+EasyOCR: 145ì\n",
      "ğŸ¯ íŒì •: ë³´ìœ¡ âœ…\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 5: ë¡œë“œëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ =====\n",
    "test_path2 = r\"C:\\Users\\ehjun\\Documents\\ìº¡ìŠ¤í†¤ë””ìì¸\\ìë™ë¬¸ì„œì¸ì‹ëª¨ë¸\\data\\pdf\\ë³´ìœ¡ìê²©ì¦\\ë³´ìœ¡êµì‚¬_ìê²©ì¦.pdf.pdf\"\n",
    "\n",
    "result2 = loaded_classifier.classify_childcare_with_rules(test_path2)\n",
    "print(\"ğŸ¯ íŒì •:\", \"ë³´ìœ¡ âœ…\" if result2[\"verdict\"] else \"ì¼ë°˜ âŒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0552f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capston",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
